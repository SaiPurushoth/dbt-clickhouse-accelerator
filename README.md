# ğŸš€ Open Source Analytics Accelerator: 
# ğŸ“Š Airflow-Cosmos + dbt + ClickHouse

A complete **end-to-end analytics platform** that integrates:

- **ClickHouse** â†’ Fast OLAP database
- **dbt** â†’ Transformations (bronze â†’ silver â†’ gold layers)
- **Airflow + Cosmos** â†’ Orchestration
- **Astro (Docker)** â†’ Local Airflow runtime

This project demonstrates a **Food Truck Analytics** pipeline: from raw orders, menus, trucks, and sessions â†’ to cleaned marts with daily sales, funnels, and top locations.

---

## âœ¨ Features

- âœ… Ready-to-use **local stack** with Astro CLI
- âœ… **Cosmos integration** to run dbt commands inside Airflow DAGs
- âœ… **ClickHouse adapter for dbt** preconfigured
- âœ… **Food Truck demo models** with seeds, tests, and marts
- âœ… Modular folder structure for DAGs, dbt project, and configs

---

## ğŸ—ï¸ Architecture

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚         Astro (Docker)            â”‚
            â”‚  Airflow: Webserver | Scheduler   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      Cosmos Operator
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”
                     â”‚     dbt       â”‚
                     â”‚ (models/tests)â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---â”
                     â”‚  ClickHouse     â”‚
                     â”‚ rawâ†’bronzeâ†’gold |
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---â”˜
```

---

## ğŸ“‚ Repository Layout

```
.
â”œâ”€ dags/                     # Airflow DAGs (Cosmos dbt orchestration + helpers)
â”œâ”€ analytics/                # dbt project (models, seeds, profiles)
â”œâ”€ include/                  # Assets available to Airflow at runtime
â”œâ”€ tests/                    # DAG/unit tests with pytest
â”œâ”€ Dockerfile                # Astro base image + dependencies
â”œâ”€ requirements.txt          # Python deps (dbt, cosmos, clickhouse-connect, etc.)
â”œâ”€ docker-compose.override.yml
â””â”€ README.md
```

---

## âš¡ Quickstart

### 1. Install prerequisites

- [Docker](https://docs.docker.com/get-docker/)
- [Astro CLI](https://www.astronomer.io/docs/astro/cli/install-cli)

### 2. Start Airflow (Astro)

```bash
# from repo root
astro dev start
```

UI available at ğŸ‘‰ [http://localhost:8080](http://localhost:8080)  
(Default: user `admin` / pwd `admin`)

### 3. Configure connections

In Airflow UI â†’ _Admin â†’ Connections_:

### 4. Configure dbt

Edit `analytics/profiles.yml`:

### 5. Run pipeline

- Trigger DAG: `food_truck_data_pipeline`

---

## Injestion Pipeline Setup

- Config Driven: 
- For DDL statements and S3 path
- Edit `dags/config/tables_info.py`:

**ğŸ”§ .evv file :**

**CLICKHOUSE CONFIGURATION**

CLICKHOUSE_HOST=host.docker.internal

CLICKHOUSE_PORT=8123

CLICKHOUSE_USER=

CLICKHOUSE_PASSWORD=

CLICKHOUSE_DATABASE=

**AWS S3 CONFIGURATION**

AWS_ACCESS_KEY_ID=

AWS_SECRET_ACCESS_KEY=

AWS_REGION=us-east-1

S3_BUCKET=your-bucket

S3_PREFIX=food_truck/raw/


## ğŸ½ï¸ Food Truck Demo Models

**Raw (landing):**

- `raw_truck`, `raw_menu`, `raw_order`, `raw_location`,..

![alt text](images/1*CgPOptqGehCjUiT1UcydXg.webp)

**Silver (conformed):**

- - `stg_truck`, `stg_menu`, `stg_order`, `stg_location`,..

![alt text](images/1*6SWDcO5dhoM_EnfYzyrnZw.webp)

**intermediate (conformed):**

- - `int_customer_segmented`, `int_menu_profitability`,

**Gold (marts):**

- `mart_daily_sales` â†’ sales KPIs
- `mart_top_locations` â†’ best-performing areas
- `mart_funnel` â†’ order funnel analysis
- `mart_peak_hours` â†’ hourly sales

![alt text](images/1*DeAIiwRLu79CiTrWdcMtDQ.webp)

---

## ğŸ§ª Testing (optional)

- **dbt tests**: run inside Airflow task `dbt_test`
- **DAG tests**:

```bash
pytest -q
```

---

## ğŸ”§ Development workflow

```bash
# stop services
astro dev stop

# rebuild with deps
astro dev restart --rebuild

# view logs
astro dev logs -f
```

---

## ğŸš€ Roadmap

- [ ] Add Kafka â†’ ClickHouse streaming ingest example
- [ ] CI pipeline with `dbt build --warn-error`
- [ ] Preconfigured dashboards - `superset`

---

## ğŸ“œ License

MIT (or update to your orgâ€™s preference)

---

## ğŸ™Œ Credits

Built with â¤ï¸ by [SaiPurushoth](https://github.com/SaiPurushoth)  
Powered by **Astro**, **Airflow**, **dbt**, **Cosmos**, and **ClickHouse**
